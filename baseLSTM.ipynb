{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange, repeat, einsum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "#import torchcd \n",
    "from torch import nn\n",
    "from src.learner import Learner\n",
    "from src.callback.core import *\n",
    "from src.callback.tracking import *\n",
    "from src.callback.scheduler import *\n",
    "from src.callback.patch_mask import *\n",
    "from src.callback.transforms import *\n",
    "from src.metrics import *\n",
    "from datautils import get_dls\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import datetime\n",
    "from functools import partial\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "from timm.utils import accuracy, AverageMeter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from xlstm1.xlstm_block_stack import xLSTMBlockStack, xLSTMBlockStackConfig\n",
    "\n",
    "from xlstm1.blocks.mlstm.block import mLSTMBlockConfig\n",
    "from xlstm1.blocks.slstm.block import sLSTMBlockConfig\n",
    "\n",
    "from myxlstm import xlstm\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the time feature classes and functions\n",
    "\n",
    "class TimeFeature:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class SecondOfMinute(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class MinuteOfHour(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class HourOfDay(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfWeek(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfMonth(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfYear(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "class MonthOfYear(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "class WeekOfYear(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "\n",
    "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "def time_features(dates, freq='D'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D:/ƒêACN/xLSTMTime-main/Datasets/forex/EURUSD_03_23.csv',index_col='date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>MA_10</th>\n",
       "      <th>MACD</th>\n",
       "      <th>MACD_signal</th>\n",
       "      <th>ROC_2</th>\n",
       "      <th>Momentum_4</th>\n",
       "      <th>RSI_10</th>\n",
       "      <th>BB_high</th>\n",
       "      <th>BB_low</th>\n",
       "      <th>BB_middle</th>\n",
       "      <th>CCI_20</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003-01-01</th>\n",
       "      <td>1.0492</td>\n",
       "      <td>1.0503</td>\n",
       "      <td>1.0467</td>\n",
       "      <td>1.0480</td>\n",
       "      <td>601</td>\n",
       "      <td>1.03691</td>\n",
       "      <td>0.011034</td>\n",
       "      <td>0.008843</td>\n",
       "      <td>0.038183</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>84.505589</td>\n",
       "      <td>1.054167</td>\n",
       "      <td>0.998433</td>\n",
       "      <td>1.026300</td>\n",
       "      <td>141.214955</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-02</th>\n",
       "      <td>1.0479</td>\n",
       "      <td>1.0500</td>\n",
       "      <td>1.0335</td>\n",
       "      <td>1.0366</td>\n",
       "      <td>8880</td>\n",
       "      <td>1.03786</td>\n",
       "      <td>0.010255</td>\n",
       "      <td>0.009125</td>\n",
       "      <td>-1.172657</td>\n",
       "      <td>-0.0077</td>\n",
       "      <td>59.073128</td>\n",
       "      <td>1.053598</td>\n",
       "      <td>1.002632</td>\n",
       "      <td>1.028115</td>\n",
       "      <td>83.305890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.497260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-03</th>\n",
       "      <td>1.0362</td>\n",
       "      <td>1.0440</td>\n",
       "      <td>1.0340</td>\n",
       "      <td>1.0428</td>\n",
       "      <td>9109</td>\n",
       "      <td>1.03940</td>\n",
       "      <td>0.010023</td>\n",
       "      <td>0.009305</td>\n",
       "      <td>-0.496183</td>\n",
       "      <td>-0.0048</td>\n",
       "      <td>65.370919</td>\n",
       "      <td>1.054475</td>\n",
       "      <td>1.005125</td>\n",
       "      <td>1.029800</td>\n",
       "      <td>75.442338</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.433333</td>\n",
       "      <td>-0.494521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-06</th>\n",
       "      <td>1.0420</td>\n",
       "      <td>1.0497</td>\n",
       "      <td>1.0416</td>\n",
       "      <td>1.0447</td>\n",
       "      <td>9405</td>\n",
       "      <td>1.04121</td>\n",
       "      <td>0.009878</td>\n",
       "      <td>0.009420</td>\n",
       "      <td>0.781401</td>\n",
       "      <td>-0.0042</td>\n",
       "      <td>67.095012</td>\n",
       "      <td>1.055373</td>\n",
       "      <td>1.007607</td>\n",
       "      <td>1.031490</td>\n",
       "      <td>97.651698</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.486301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-07</th>\n",
       "      <td>1.0444</td>\n",
       "      <td>1.0457</td>\n",
       "      <td>1.0395</td>\n",
       "      <td>1.0405</td>\n",
       "      <td>9297</td>\n",
       "      <td>1.04223</td>\n",
       "      <td>0.009317</td>\n",
       "      <td>0.009399</td>\n",
       "      <td>-0.220560</td>\n",
       "      <td>-0.0075</td>\n",
       "      <td>59.784293</td>\n",
       "      <td>1.054621</td>\n",
       "      <td>1.011649</td>\n",
       "      <td>1.033135</td>\n",
       "      <td>66.162912</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.483562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              open    high     low   close  volume    MA_10      MACD  \\\n",
       "date                                                                    \n",
       "2003-01-01  1.0492  1.0503  1.0467  1.0480     601  1.03691  0.011034   \n",
       "2003-01-02  1.0479  1.0500  1.0335  1.0366    8880  1.03786  0.010255   \n",
       "2003-01-03  1.0362  1.0440  1.0340  1.0428    9109  1.03940  0.010023   \n",
       "2003-01-06  1.0420  1.0497  1.0416  1.0447    9405  1.04121  0.009878   \n",
       "2003-01-07  1.0444  1.0457  1.0395  1.0405    9297  1.04223  0.009317   \n",
       "\n",
       "            MACD_signal     ROC_2  Momentum_4     RSI_10   BB_high    BB_low  \\\n",
       "date                                                                           \n",
       "2003-01-01     0.008843  0.038183      0.0112  84.505589  1.054167  0.998433   \n",
       "2003-01-02     0.009125 -1.172657     -0.0077  59.073128  1.053598  1.002632   \n",
       "2003-01-03     0.009305 -0.496183     -0.0048  65.370919  1.054475  1.005125   \n",
       "2003-01-06     0.009420  0.781401     -0.0042  67.095012  1.055373  1.007607   \n",
       "2003-01-07     0.009399 -0.220560     -0.0075  59.784293  1.054621  1.011649   \n",
       "\n",
       "            BB_middle      CCI_20  day_of_week  day_of_month  day_of_year  \n",
       "date                                                                       \n",
       "2003-01-01   1.026300  141.214955    -0.166667     -0.500000    -0.500000  \n",
       "2003-01-02   1.028115   83.305890     0.000000     -0.466667    -0.497260  \n",
       "2003-01-03   1.029800   75.442338     0.166667     -0.433333    -0.494521  \n",
       "2003-01-06   1.031490   97.651698    -0.500000     -0.333333    -0.486301  \n",
       "2003-01-07   1.033135   66.162912    -0.333333     -0.300000    -0.483562  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "time_feats = time_features(data.index, 'D')  # Generate time features assuming daily frequency\n",
    "time_features_names = [ 'day_of_week', 'day_of_month', 'day_of_year']\n",
    "time_features_df = pd.DataFrame(time_feats.T, columns=time_features_names, index=data.index)\n",
    "data_with_time_features = pd.concat([data, time_features_df], axis=1)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data_with_time_features)\n",
    "\n",
    "# Display the first few rows of the new dataframe with time features added\n",
    "data_with_time_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4432, 18), (831, 18), (278, 18))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the split sizes\n",
    "train_size = 0.80\n",
    "validation_size = 0.15\n",
    "test_size = 0.05\n",
    "\n",
    "# Calculate the actual split sizes for validation and test relative to remaining data after train split\n",
    "validation_size_adjusted = validation_size / (1 - train_size)\n",
    "\n",
    "# Split the data into train and temporary set (validation + test)\n",
    "data_train, data_temp = train_test_split(data_with_time_features, train_size=train_size, shuffle=False)\n",
    "\n",
    "# Split the temporary set into validation and test\n",
    "data_validation, data_test = train_test_split(data_temp, train_size=validation_size_adjusted, shuffle=False)\n",
    "\n",
    "# Check the sizes of each dataset\n",
    "data_train.shape, data_validation.shape, data_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.014228549785912037\n",
      "Epoch 2, Loss: 0.01576698198914528\n",
      "Epoch 3, Loss: 0.013516672886908054\n",
      "Epoch 4, Loss: 0.010585346259176731\n",
      "Epoch 5, Loss: 0.012935062870383263\n",
      "Epoch 6, Loss: 0.011925282888114452\n",
      "Epoch 7, Loss: 0.008378230966627598\n",
      "Epoch 8, Loss: 0.019651122391223907\n",
      "Epoch 9, Loss: 0.011906091123819351\n",
      "Epoch 10, Loss: 0.018057970330119133\n",
      "Epoch 11, Loss: 0.00950110238045454\n",
      "Epoch 12, Loss: 0.008659851737320423\n",
      "Epoch 13, Loss: 0.009669283404946327\n",
      "Epoch 14, Loss: 0.00932097528129816\n",
      "Epoch 15, Loss: 0.01341534499078989\n",
      "Epoch 16, Loss: 0.013502925634384155\n",
      "Epoch 17, Loss: 0.012190116569399834\n",
      "Epoch 18, Loss: 0.02649473026394844\n",
      "Epoch 19, Loss: 0.020379364490509033\n",
      "Epoch 20, Loss: 0.017293579876422882\n",
      "Epoch 21, Loss: 0.018149886280298233\n",
      "Epoch 22, Loss: 0.013418524526059628\n",
      "Epoch 23, Loss: 0.01854548044502735\n",
      "Epoch 24, Loss: 0.02315455675125122\n",
      "Epoch 25, Loss: 0.027532897889614105\n",
      "Epoch 26, Loss: 0.010997777804732323\n",
      "Epoch 27, Loss: 0.014208221808075905\n",
      "Epoch 28, Loss: 0.011296327225863934\n",
      "Epoch 29, Loss: 0.012882981449365616\n",
      "Epoch 30, Loss: 0.009718695655465126\n",
      "Epoch 31, Loss: 0.01914004050195217\n",
      "Epoch 32, Loss: 0.023076388984918594\n",
      "Epoch 33, Loss: 0.011621086858212948\n",
      "Epoch 34, Loss: 0.014332901686429977\n",
      "Epoch 35, Loss: 0.020589414983987808\n",
      "Epoch 36, Loss: 0.01594514772295952\n",
      "Epoch 37, Loss: 0.024631189182400703\n",
      "Epoch 38, Loss: 0.017840998247265816\n",
      "Epoch 39, Loss: 0.01421353779733181\n",
      "Epoch 40, Loss: 0.015358497388660908\n",
      "Epoch 41, Loss: 0.019484011456370354\n",
      "Epoch 42, Loss: 0.012268044985830784\n",
      "Epoch 43, Loss: 0.006986056454479694\n",
      "Epoch 44, Loss: 0.01490234024822712\n",
      "Epoch 45, Loss: 0.012011121027171612\n",
      "Epoch 46, Loss: 0.014410870149731636\n",
      "Epoch 47, Loss: 0.01484496146440506\n",
      "Epoch 48, Loss: 0.015369866043329239\n",
      "Epoch 49, Loss: 0.022278543561697006\n",
      "Epoch 50, Loss: 0.012255195528268814\n",
      "Test MSE: 0.057468737165133156\n",
      "Test MAE: 0.23913249373435974\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.functional import mse_loss, l1_loss\n",
    "\n",
    "# Function to create Tensors from numpy arrays for PyTorch\n",
    "def create_tensors(data):\n",
    "    return torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "# Reshape data and convert to PyTorch tensors\n",
    "X_train = create_tensors(data_train.drop('close', axis=1).values).reshape(-1, 1, data_train.shape[1] - 1)\n",
    "y_train = create_tensors(data_train['close'].values).reshape(-1, 1)\n",
    "X_validation = create_tensors(data_validation.drop('close', axis=1).values).reshape(-1, 1, data_validation.shape[1] - 1)\n",
    "y_validation = create_tensors(data_validation['close'].values).reshape(-1, 1)\n",
    "X_test = create_tensors(data_test.drop('close', axis=1).values).reshape(-1, 1, data_test.shape[1] - 1)\n",
    "y_test = create_tensors(data_test['close'].values).reshape(-1, 1)\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Model instantiation\n",
    "model = LSTMModel(input_dim=X_train.shape[2], hidden_dim=50)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32  # You can adjust this value as needed\n",
    "\n",
    "# Prepare data loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_dataset = TensorDataset(X_validation, y_validation)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# Training the model\n",
    "def train_model(num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_losses.append(loss.item())\n",
    "        avg_loss = sum(val_losses) / len(val_losses)\n",
    "    print(f'Validation Loss: {avg_loss}')\n",
    "\n",
    "def evaluate_model_on_test():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        test_losses_mse = []\n",
    "        test_losses_mae = []\n",
    "        \n",
    "        # Prepare the test loader\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate MSE and MAE\n",
    "            loss_mse = mse_loss(outputs, targets)\n",
    "            loss_mae = l1_loss(outputs, targets)\n",
    "            \n",
    "            test_losses_mse.append(loss_mse.item())\n",
    "            test_losses_mae.append(loss_mae.item())\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_mse = sum(test_losses_mse) / len(test_losses_mse)\n",
    "        avg_mae = sum(test_losses_mae) / len(test_losses_mae)\n",
    "    \n",
    "    print(f'Test MSE: {avg_mse}')\n",
    "    print(f'Test MAE: {avg_mae}')\n",
    "\n",
    "# Run training and evaluation\n",
    "train_model(50)\n",
    "evaluate_model_on_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] library\n",
      "ipykernel_launcher.py: error: the following arguments are required: library\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import importlib\n",
    "\n",
    "def check_library_version(library_name):\n",
    "    try:\n",
    "        lib = importlib.import_module(library_name)\n",
    "        print(f\"The version of {library_name} is {lib.__version__}\")\n",
    "    except ImportError:\n",
    "        print(f\"Library {library_name} is not installed.\")\n",
    "    except AttributeError:\n",
    "        print(f\"Version information is not available for {library_name}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Check the version of a Python library.\")\n",
    "    parser.add_argument(\"library\", type=str, help=\"Name of the library to check\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    check_library_version(args.library)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.13.1\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\n",
      "Requires: tensorflow-intel\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
